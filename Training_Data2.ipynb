{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "90225f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import models, transforms\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset\n",
    "from tqdm import tqdm\n",
    "import collections\n",
    "import shutil\n",
    "import torch.nn.functional as F\n",
    "from torchvision import models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d2b69bab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define device\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "877e0061",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define constants\n",
    "IMG_HEIGHT, IMG_WIDTH = 224, 224\n",
    "BATCH_SIZE = 8\n",
    "NUM_EPOCHS = 500\n",
    "data_dir = 'C:\\\\Users\\\\ASUS\\\\Desktop\\\\DIFL\\\\22march DIFL'\n",
    "shenzen_dir = os.path.join(data_dir, '0')\n",
    "usa_dir = os.path.join(data_dir, '1')\n",
    "MODEL_PATH_GC = os.path.join(data_dir, 'Models', 'modelGC.pth')\n",
    "MODEL_PATH_G = os.path.join(data_dir, 'Models', 'modelG.pth')\n",
    "MODEL_PATH_D = os.path.join(data_dir, 'Models', 'modelC.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d60f518b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((IMG_HEIGHT, IMG_WIDTH)),\n",
    "    transforms.Grayscale(),  # Convert to grayscale\n",
    "    transforms.Lambda(lambda x: x.convert('RGB'))  # Convert grayscale to RGB\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "e0976931",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define dataset class\n",
    "class TBDataset(Dataset):\n",
    "    def __init__(self, root_dir, label_type, distribution_type, transform=None):\n",
    "        self.transform = transform\n",
    "        self.label_type = label_type  # 'classification_label' or 'domain_label'\n",
    "        self.distribution_type = distribution_type  # 'source' or 'target'\n",
    "        \n",
    "        self.folders_list = os.listdir(root_dir)\n",
    "        self.Img_list = []\n",
    "        for folder in self.folders_list:\n",
    "            self.Img_list.extend(\n",
    "                [os.path.join(root_dir, folder, path) for path in os.listdir(os.path.join(root_dir, folder))]\n",
    "            )\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Img_list)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        image = Image.open(self.Img_list[index])\n",
    "        image = image.convert(\"L\")\n",
    "        label = None\n",
    "        if self.label_type == 'classification_label':\n",
    "            label = int(self.Img_list[index].split(os.path.sep)[-2])\n",
    "        elif self.label_type == 'domain_label':\n",
    "            if self.distribution_type == 'source':\n",
    "                label = 0\n",
    "            elif self.distribution_type == 'target':\n",
    "                label = 1\n",
    "        if self.transform is not None:\n",
    "            image = self.transform(image)\n",
    "        x = transforms.ToTensor()(image)\n",
    "        y = np.array(label)\n",
    "        y = torch.from_numpy(y)\n",
    "        y = y.type(torch.LongTensor)\n",
    "        return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cdb39bc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to create a balanced subset\n",
    "def create_subset(dataset, data_limit, num_classes=2):\n",
    "    data_indices = []\n",
    "    per_class_data_limit = data_limit // num_classes\n",
    "    target_counter = collections.Counter()\n",
    "    with tqdm(total=len(dataset)) as bar:\n",
    "        for idx, data in enumerate(dataset):\n",
    "            if idx == len(dataset) - 1:\n",
    "                for i in range(data_limit - per_class_data_limit * num_classes):\n",
    "                    data_indices.append(idx)\n",
    "            target = data[1].item()\n",
    "            target_counter[target] += 1\n",
    "            if target_counter[target] <= per_class_data_limit:\n",
    "                data_indices.append(idx)\n",
    "            bar.update(1)\n",
    "\n",
    "    sub_dataset = torch.utils.data.Subset(dataset, data_indices)\n",
    "    return sub_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "0c3dce86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to split dataset in a stratified manner\n",
    "def stratify_split(dataset, split):\n",
    "    train_indices = []\n",
    "    test_indices = []\n",
    "    target_stat = collections.Counter()\n",
    "    with tqdm(total=len(dataset)) as bar:\n",
    "        for idx, data in enumerate(dataset):\n",
    "            target = data[1].item()\n",
    "            target_stat[target] += 1\n",
    "        bar.update(1)\n",
    "\n",
    "    for k in target_stat.keys():\n",
    "        target_stat[k] = int(target_stat[k] * split / 100.0)\n",
    "    target_counter = collections.Counter()\n",
    "    with tqdm(total=len(dataset)) as bar:\n",
    "        for idx, data in enumerate(dataset):\n",
    "            target = data[1].item()\n",
    "            target_counter[target] += 1\n",
    "            if target_counter[target] <= target_stat[target]:\n",
    "                train_indices.append(idx)\n",
    "            else:\n",
    "                test_indices.append(idx)\n",
    "            bar.update(1)\n",
    "\n",
    "    train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "    test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "    return train_dataset, test_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "39721104",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to count number of parameters in a model\n",
    "def count_num_param(model):\n",
    "    return sum(p.numel() for p in model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "cb616093",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define function to print number of parameters with formatting\n",
    "def print_param(num_param):\n",
    "    num_param = str(num_param)\n",
    "    ret = []\n",
    "    length = len(num_param)\n",
    "    for i in range(length):\n",
    "        idx = i + 1\n",
    "        if idx > 1 and (idx - 1) % 3 == 0:\n",
    "            ret.append(',')\n",
    "        ret.append(num_param[length - idx])\n",
    "    ret.reverse()\n",
    "    ret = ''.join(ret)\n",
    "    temp_a = '## Number of parameters: ' + ret + ' ##'\n",
    "    temp_b = '#' * len(temp_a)\n",
    "    columns = shutil.get_terminal_size().columns\n",
    "    print(f'{temp_b}'.center(columns))\n",
    "    print(f'{temp_a}'.center(columns))\n",
    "    print(f'{temp_b}'.center(columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "d3530380",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(torch.nn.Module):\n",
    "    def __init__(self, num_of_filters):\n",
    "        super(Generator, self).__init__()\n",
    "        self.num_of_filters = num_of_filters\n",
    "        self.Resnet50_model = models.resnet50(pretrained=False)\n",
    "        # Modify the first convolutional layer to accept 1 input channel instead of 3\n",
    "        self.Resnet50_model.conv1 = nn.Conv2d(1, 64, kernel_size=7, stride=2, padding=3, bias=False)\n",
    "        # Remove the max pooling layer after the first convolutional block\n",
    "        self.Resnet50_model.maxpool = nn.Identity()\n",
    "        # Remove the final average pooling and fully connected layers\n",
    "        self.Resnet50_model.fc = nn.Identity()\n",
    "\n",
    "        # Initialize the remaining layers of the Generator\n",
    "        self.A_UpConv_1 = nn.ConvTranspose2d(2048, 1024, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.A_activation_1 = torch.nn.LeakyReLU(negative_slope=0.2, inplace=False)\n",
    "        self.A_UpConv_2 = nn.ConvTranspose2d(1024, self.num_of_filters, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.A_activation_2 = torch.nn.LeakyReLU(negative_slope=0.2, inplace=False)\n",
    "        self.A_conv = nn.Conv2d(self.num_of_filters, self.num_of_filters, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.A_activation_3 = torch.nn.LeakyReLU(negative_slope=0.2, inplace=False)\n",
    "\n",
    "        self.B_UpConv_1 = nn.ConvTranspose2d(self.num_of_filters, self.num_of_filters, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.B_activation_1 = torch.nn.LeakyReLU(negative_slope=0.2, inplace=False)\n",
    "        self.B_conv = nn.Conv2d(self.num_of_filters, self.num_of_filters, kernel_size=3, stride=1, padding=1, bias=False)\n",
    "        self.B_activation_3 = torch.nn.LeakyReLU(negative_slope=0.2, inplace=False)\n",
    "\n",
    "        self.C_UpConv_1 = nn.ConvTranspose2d(self.num_of_filters, self.num_of_filters, kernel_size=3, stride=2, padding=1, output_padding=1)\n",
    "        self.C_activation_1 = torch.nn.LeakyReLU(negative_slope=0.2, inplace=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Ensure that the input images are correctly resized and processed as grayscale\n",
    "        x = F.interpolate(x.unsqueeze(1), size=(224, 224))  # Add channel dimension and interpolate\n",
    "        # Forward pass through the modified ResNet50 model\n",
    "        x = self.Resnet50_model(x)\n",
    "        # Reshape the output to include spatial dimensions\n",
    "        x = x.view(x.size(0), -1, 1, 1)  # Assume the spatial dimensions are 1x1\n",
    "        # Check the shape of the output after reshaping\n",
    "        print(\"Shape after ResNet50 and reshaping:\", x.shape)\n",
    "        # Forward pass through the remaining layers of the Generator\n",
    "        x = self.A_UpConv_1(x)\n",
    "        x = self.A_activation_1(x)\n",
    "        x = self.A_UpConv_2(x)\n",
    "        x = self.A_activation_2(x)\n",
    "        x = self.A_conv(x)\n",
    "        x = self.A_activation_3(x)\n",
    "        x = self.B_UpConv_1(x)\n",
    "        x = self.B_activation_1(x)\n",
    "        x = self.B_conv(x)\n",
    "        x = self.B_activation_3(x)\n",
    "        x = self.C_UpConv_1(x)\n",
    "        x = self.C_activation_1(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a4aaa8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the Discriminator class\n",
    "class Discriminator(torch.nn.Module):\n",
    "    def __init__(self, num_of_filters):\n",
    "        super(Discriminator, self).__init__()\n",
    "        self.num_of_filters = num_of_filters\n",
    "        self.vgg19_model = models.vgg19(pretrained=False)\n",
    "        self.vgg19_model.features[0] = nn.Conv2d(512, 64, 3, 1, 1)\n",
    "        self.vgg19_model = nn.Sequential(*list(self.vgg19_model.children())[:-2])\n",
    "        self.Linear_1 = nn.Linear(self.num_of_filters, 256)\n",
    "        self.Activation_1 = torch.nn.LeakyReLU(negative_slope=0.2, inplace=False)\n",
    "        self.Linear_2 = nn.Linear(256, 128)\n",
    "        self.Activation_2 = torch.nn.LeakyReLU(negative_slope=0.2, inplace=False)\n",
    "        self.Linear_3 = nn.Linear(128, 64)\n",
    "        self.Activation_3 = torch.nn.LeakyReLU(negative_slope=0.2, inplace=False)\n",
    "        self.Linear_final = nn.Linear(64, 2)\n",
    "        self.Activation_final = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vgg19_model(x)\n",
    "        x = nn.Flatten()(x)\n",
    "        x = self.Linear_1(x)\n",
    "        x = self.Activation_1(x)\n",
    "        x = self.Linear_2(x)\n",
    "        x = self.Activation_2(x)\n",
    "        x = self.Linear_3(x)\n",
    "        x = self.Activation_3(x)\n",
    "        x = self.Linear_final(x)\n",
    "        x = self.Activation_final(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6899322f",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "939f96fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision.models as models\n",
    "\n",
    "class Classifier(torch.nn.Module):\n",
    "    def __init__(self, num_of_filters):\n",
    "        super(Classifier, self).__init__()\n",
    "        self.num_of_filters = num_of_filters\n",
    "        self.vgg19_model = models.vgg19(pretrained=False)\n",
    "        self.vgg19_model.features[0] = nn.Conv2d(512, 64, 3, 1, 1)\n",
    "        self.vgg19_model = nn.Sequential(*list(self.vgg19_model.children())[:-1])  # Remove last pooling layer\n",
    "        self.adaptive_pooling = nn.AdaptiveAvgPool2d((1, 1))  # Add adaptive pooling\n",
    "        self.Linear_1 = nn.Linear(self.num_of_filters, 256)\n",
    "        self.Activation_1 = torch.nn.LeakyReLU(negative_slope=0.2, inplace=False)\n",
    "        self.Linear_2 = nn.Linear(256, 128)\n",
    "        self.Activation_2 = torch.nn.LeakyReLU(negative_slope=0.2, inplace=False)\n",
    "        self.Linear_3 = nn.Linear(128, 64)\n",
    "        self.Activation_3 = torch.nn.LeakyReLU(negative_slope=0.2, inplace=False)\n",
    "        self.Linear_final = nn.Linear(64, 2)\n",
    "        self.Activation_final = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.vgg19_model(x)\n",
    "        print(\"Shape after VGG19 model:\", x.shape)\n",
    "        # Apply adaptive pooling\n",
    "        x = self.adaptive_pooling(x)\n",
    "        print(\"Shape after adaptive pooling:\", x.shape)\n",
    "        x = nn.Flatten()(x)\n",
    "        print(\"Shape after flattening:\", x.shape)\n",
    "        x = self.Linear_1(x)\n",
    "        print(\"Shape after Linear_1:\", x.shape)\n",
    "        x = self.Activation_1(x)\n",
    "        print(\"Shape after Activation_1:\", x.shape)\n",
    "        x = self.Linear_2(x)\n",
    "        print(\"Shape after Linear_2:\", x.shape)\n",
    "        x = self.Activation_2(x)\n",
    "        print(\"Shape after Activation_2:\", x.shape)\n",
    "        x = self.Linear_3(x)\n",
    "        print(\"Shape after Linear_3:\", x.shape)\n",
    "        x = self.Activation_3(x)\n",
    "        print(\"Shape after Activation_3:\", x.shape)\n",
    "        x = self.Linear_final(x)\n",
    "        print(\"Shape after Linear_final:\", x.shape)\n",
    "        x = self.Activation_final(x)\n",
    "        print(\"Shape after Activation_final:\", x.shape)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "ff506888",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize models\n",
    "G = Generator(512).to(device)\n",
    "D = Discriminator(512).to(device)\n",
    "C = Classifier(512).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "9e568f6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     ######################################                     \n",
      "                     ## Number of parameters: 56,534,464 ##                     \n",
      "                     ######################################                     \n",
      "                     ######################################                     \n",
      "                     ## Number of parameters: 20,490,178 ##                     \n",
      "                     ######################################                     \n",
      "                     ######################################                     \n",
      "                     ## Number of parameters: 20,490,178 ##                     \n",
      "                     ######################################                     \n"
     ]
    }
   ],
   "source": [
    "# Print number of parameters for each model\n",
    "print_param(count_num_param(G))\n",
    "print_param(count_num_param(D))\n",
    "print_param(count_num_param(C))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "66f75069",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define BCE loss\n",
    "criterion = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "922338cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define optimizers\n",
    "Params_Classification = list(G.parameters()) + list(C.parameters())\n",
    "Optimizer_Classification = optim.Adam(Params_Classification, lr=0.0002, betas=(0.5, 0.999))\n",
    "Optimizer_DomainInvariance_G = optim.Adam(G.parameters(), lr=0.0002, betas=(0.5, 0.999))\n",
    "Optimizer_DomainInvariance_D = optim.Adam(D.parameters(), lr=0.0002, betas=(0.5, 0.999))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "c04978ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define training function\n",
    "def training(diseases_classification_trainset, domain_classification_trainset,\n",
    "             MODEL_PATH_GC, MODEL_PATH_G, MODEL_PATH_D, device, num_epochs):\n",
    "    G.to(device)\n",
    "    C.to(device)\n",
    "    D.to(device)\n",
    "    G.eval()\n",
    "    C.eval()\n",
    "    D.eval()\n",
    "    for epoch in range(num_epochs):\n",
    "        for batch_number, example in enumerate(diseases_classification_trainset):\n",
    "            X_s, y_s = example\n",
    "            X_s, y_s = X_s.to(device), y_s.to(device)\n",
    "            G_X_s = G(X_s)\n",
    "            y_hat_s = C(G_X_s)\n",
    "            l_c = criterion(y_hat_s, y_s)\n",
    "            Optimizer_Classification.zero_grad()\n",
    "            l_c.backward()\n",
    "            Optimizer_Classification.step()\n",
    "        torch.save({\n",
    "                'model_state_dict': C.state_dict(),\n",
    "                'optim_state_dict': Optimizer_Classification.state_dict(),\n",
    "                'epoch': epoch\n",
    "        }, MODEL_PATH_GC)\n",
    "        for batch_number, example in enumerate(domain_classification_trainset):\n",
    "            X, d = example\n",
    "            X, d = X.to(device), d.to(device)\n",
    "            G_X = G(X)\n",
    "            d_hat = D(G_X)\n",
    "            l_d = criterion(d_hat, d)\n",
    "            d_gen = torch.full(d.shape, 0.5).to(device)\n",
    "            l_g = criterion(d_hat, d_gen)\n",
    "\n",
    "            Optimizer_DomainInvariance_G.zero_grad()\n",
    "            Optimizer_DomainInvariance_D.zero_grad()\n",
    "            l_d.backward()\n",
    "            l_g.backward()\n",
    "            Optimizer_DomainInvariance_G.step()\n",
    "            Optimizer_DomainInvariance_D.step()\n",
    "        torch.save({\n",
    "                'model_state_dict': G.state_dict(),\n",
    "                'optim_state_dict': Optimizer_DomainInvariance_G.state_dict(),\n",
    "                'epoch': epoch\n",
    "        }, MODEL_PATH_G)\n",
    "        torch.save({\n",
    "                'model_state_dict': D.state_dict(),\n",
    "                'optim_state_dict': Optimizer_DomainInvariance_D.state_dict(),\n",
    "                'epoch': epoch\n",
    "        }, MODEL_PATH_D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "348781d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define evaluation function\n",
    "def evaluate(dataset_loader, model, device):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    with torch.no_grad():\n",
    "        for data in dataset_loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    accuracy = 100 * correct / total\n",
    "    return accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "f3f99234",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define datasets\n",
    "shenzen_dataset = TBDataset(shenzen_dir, label_type='classification_label', distribution_type='source', transform=transform)\n",
    "usa_dataset = TBDataset(usa_dir, label_type='classification_label', distribution_type='target', transform=transform)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "00439a3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 662/662 [01:05<00:00, 10.03it/s]\n",
      "100%|██████████| 138/138 [00:35<00:00,  3.93it/s]\n"
     ]
    }
   ],
   "source": [
    "# Create balanced subsets\n",
    "shenzen_balanced = create_subset(shenzen_dataset, 2000)\n",
    "usa_balanced = create_subset(usa_dataset, 2000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "37417c5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/662 [01:03<11:38:37, 63.42s/it]\n",
      "100%|██████████| 662/662 [01:05<00:00, 10.18it/s]\n",
      "  1%|          | 1/138 [00:32<1:13:58, 32.40s/it]\n",
      "100%|██████████| 138/138 [00:33<00:00,  4.17it/s]\n"
     ]
    }
   ],
   "source": [
    "# Split datasets\n",
    "shenzen_trainset, shenzen_testset = stratify_split(shenzen_balanced, split=80)\n",
    "usa_trainset, usa_testset = stratify_split(usa_balanced, split=80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6769b70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate your classifier\n",
    "classifier = Classifier(num_of_filters=512)\n",
    "\n",
    "# Check the parameters of the max pooling layers\n",
    "for module in classifier.vgg19_model:\n",
    "    if isinstance(module, nn.MaxPool2d):\n",
    "        print(\"Kernel size:\", module.kernel_size)\n",
    "        print(\"Stride:\", module.stride)\n",
    "\n",
    "# Continue with the rest of your code, such as training and evaluation\n",
    "training(shenzen_trainset, usa_trainset, MODEL_PATH_GC, MODEL_PATH_G, MODEL_PATH_D, device, NUM_EPOCHS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c0cacf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Train models\n",
    "training(shenzen_trainset, usa_trainset, MODEL_PATH_GC, MODEL_PATH_G, MODEL_PATH_D, device, NUM_EPOCHS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3714e1fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained models\n",
    "C.load_state_dict(torch.load(MODEL_PATH_GC)['model_state_dict'])\n",
    "G.load_state_dict(torch.load(MODEL_PATH_G)['model_state_dict'])\n",
    "D.load_state_dict(torch.load(MODEL_PATH_D)['model_state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "db4e4af8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate models\n",
    "shenzen_testloader = torch.utils.data.DataLoader(shenzen_testset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "usa_testloader = torch.utils.data.DataLoader(usa_testset, batch_size=BATCH_SIZE, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "423663bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "shenzen_accuracy = evaluate(shenzen_testloader, C, device)\n",
    "usa_accuracy = evaluate(usa_testloader, C, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c1c40a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print results\n",
    "print(\"Accuracy on Shenzen dataset:\", shenzen_accuracy)\n",
    "print(\"Accuracy on USA dataset:\", usa_accuracy)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
